{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import (\n",
    "    SimpleDirectoryReader,\n",
    "    load_index_from_storage,\n",
    "    VectorStoreIndex,\n",
    "    StorageContext,\n",
    "    ServiceContext\n",
    ")\n",
    "from llama_index.vector_stores.faiss import FaissVectorStore\n",
    "from IPython.display import Markdown, display\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "import faiss\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.node_parser import SemanticSplitterNodeParser\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "from llama_index.core.response.notebook_utils import display_source_node\n",
    "from llama_index.core import Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM is explicitly disabled. Using MockLLM.\n"
     ]
    }
   ],
   "source": [
    "# load documents\n",
    "documents = SimpleDirectoryReader(input_files=[\"../data/thinking_machines.txt\"]).load_data()\n",
    "embed_model = OllamaEmbedding(\n",
    "    model_name=\"mxbai-embed-large:latest\"\n",
    ")\n",
    "Settings.embed_model = embed_model\n",
    "Settings.llm = None\n",
    "splitter = SemanticSplitterNodeParser(buffer_size=1, breakpoint_percentail_thresholed=95,\n",
    "                                      embed_model=embed_model)\n",
    "nodes = splitter.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  ----  Node ID: 1a1af6e9-9da2-435f-a067-25713489c386\n",
      "Text: When I was an entrepreneur about a decade ago, I landed on a\n",
      "golden use case (or so I fantasised). It is to apply Natural Language\n",
      "Processing (NLP) to convert english statements into structured facts\n",
      "(Subject-Predicate-Object) that adhere to commonly-agreed Domain\n",
      "Ontologies. For example, “Singapore’s economic inflation is estimated\n",
      "at 4.5%” can...\n",
      "1  ----  Node ID: a1eb6634-1ee4-448f-a52e-9cdb0175fa14\n",
      "Text: With the very little that I have learned about this vast\n",
      "history, I will try and categorise the different schools of thoughts\n",
      "that existed and evolved this field. If you are a sincere seeker, you\n",
      "may stitch the story also, starting with Aristotle and traversing all\n",
      "the way through the history of first order logic, inferencing,\n",
      "mathematical induc...\n",
      "2  ----  Node ID: 690be0ea-28f2-4c64-88a5-a1fab2095c5f\n",
      "Text: The AI winter(s) did not happen suddenly. There were some\n",
      "radical voices that predicted it like that of Hubert Dreyfus. They\n",
      "theorised that the magical activities like object recognition are not\n",
      "symbol manipulation problems. These are perhaps pattern matching\n",
      "problems. So here is our third category and lets call them the\n",
      "“thinking = pattern sift...\n",
      "3  ----  Node ID: da71dd67-68b0-4106-aa4b-46b0b5952670\n",
      "Text: And the 3rd and the 4th are fundamentally similar in the\n",
      "approach. Therefore we can now reduce the categories into two to move\n",
      "forward — 1/symbol manipulation and; 2/sequence transduction using\n",
      "pattern matching. Now, it is important to understand the limitations\n",
      "of both these approaches to avoid rude shocks. In the first symbol\n",
      "manipulation appr...\n",
      "4  ----  Node ID: 3f84f325-abb7-46d7-ac18-a21cf273f0b5\n",
      "Text: It gave me compelling answers that I fell for. When I read the\n",
      "actual papers later, I realised how completely wrong they were and\n",
      "felt like that same embarrassed and disappointed child.  Now, I am not\n",
      "a bitter road block prophet here.\n",
      "5  ----  Node ID: 03bc899f-11d0-4474-a041-4c82b398a5ac\n",
      "Text: All I want is for us to have a a deep understanding of what\n",
      "problems can and can-not be solved, given a particular approach. Event\n",
      "though sequence transduction approach has its flaws, it is a\n",
      "surprising, highly critical and a very useful milestone. We will learn\n",
      "from it, use this as a stepping stone and synthesise a new paradigm. I\n",
      "have some clu...\n",
      "6  ----  Node ID: dd3cbe9d-7bf1-438c-bf2a-e3957cfa5491\n",
      "Text: Before that, I want to dive into making this article a little\n",
      "more contextual for developers. My current job scope is to augment\n",
      "developer productivity and in that scope, code generation using GenAI\n",
      "is an important weapon in any developers’ armoury. And we developers\n",
      "need to know when and where to apply this technology safely!  Back to\n",
      "the 2 cha...\n",
      "7  ----  Node ID: 0d3cc5f7-3911-488a-896a-1f3f63dc12f8\n",
      "Text: As the old aphorism goes, all models are wrong; and some are\n",
      "useful. The sequence transduction approach suffers from the\n",
      "limitations of the rudimentary knowledge representation mechanism. The\n",
      "limitations of this will be amplified for the users soon (except in\n",
      "the creative use case scenarios) but the LLMs are nevertheless very\n",
      "useful and a great ...\n"
     ]
    }
   ],
   "source": [
    "for i, node in enumerate(nodes):\n",
    "    print(i, \" ---- \", node) #node.get_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "faiss_index = faiss.IndexFlatL2(1024)\n",
    "faiss_index.is_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = FaissVectorStore(faiss_index=faiss_index)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex(\n",
    "    nodes, storage_context=storage_context,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM is explicitly disabled. Using MockLLM.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 1a1af6e9-9da2-435f-a067-25713489c386<br>**Similarity:** 224.02749633789062<br>**Text:** When I was an entrepreneur about a decade ago, I landed on a golden use case (or so I fantasised). It is to apply Natural Language Processing (NLP) to convert english statements into structured facts (Subject-Predicate-Object) that adhere to commonly-agreed Domain Ontologies. For example, “Singapore’s economic inflation is estimated at 4.5%” can be expressed as a triple with Singapore as the subject, 4.5 as the object and has_inflation as the predicate (semantic relatioship). All the Subjects, Objects and Predicates can be expressed as unique URIs so that the knowledge across millions of triples describing different aspects of the same subjects and objects can be stictched together. Such fact triples can be loaded onto Semantic Datastores (RDF or Property-Graph based) and queried, again by the same natural language processing capability. Even more so, these triples can be crunched by inferencing engines to derive semantically sound new knowledge and can reason these derivations in reverse too. The ultimate thinking machine! or so I fantasised.\n",
       "\n",
       "Why this seemingly round-about approach to create a thinking machine as against the generative AI’s approach of predicting word-sequences? Why convert unstructured English to structured facts before processing it for an answer? And are there any other alternative approaches to creating thinking machines? Strangely, I have not heard much balanced critical opinions from contemporary computing historians or philosophers on this subject ever since LLMs were made public. It could also be that I am living under a certain other rock but all I hear is the whole industry monomaniacally adulating the genAI’s auto-regressive model, bullishly sidelining any reasonable criticisms. To correctly quantify the pleasantly surprising merits and some devastating demerits, we need to philosophically ponder on the humanity’s past quests on this matter and re-learn from it.<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 690be0ea-28f2-4c64-88a5-a1fab2095c5f<br>**Similarity:** 274.6891784667969<br>**Text:** The AI winter(s) did not happen suddenly. There were some radical voices that predicted it like that of Hubert Dreyfus. They theorised that the magical activities like object recognition are not symbol manipulation problems. These are perhaps pattern matching problems. So here is our third category and lets call them the “thinking = pattern sifting” school. Thanks to them, we learnt that we can’t solve a problem of recognising an object, say a chair, even if we write thousands of rules. (Although its a core component of intelligence, is recognition a sufficient capability to create thinking machines? Let us come back to this shortly)\n",
       "\n",
       "While progressing in parallel on the natural language processing frontier, we have recently landed on another possible solution to thinking machines — the LLMs. This is a variation of the pattern sifting approach, applied to word patterns in languages. By mastering the language syntax patterns sourced from an unlimited corpus and by using them as references to generate eloquent statements, these models seem to simulate thinking. This is the claim of the fourth school and lets call them the “thinking = sequence transduction” school. This also would mean that people perhaps are nothing but evolved sequence models.\n",
       "\n",
       "I am not going to dive into the second category for now.<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Settings.llm = None\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\n",
    "    \"How can semantic datastores be used in reasoning\"\n",
    ")\n",
    "for n in response.source_nodes:\n",
    "    display_source_node(n, source_length=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.storage_context.persist(persist_dir=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = FaissVectorStore.from_persist_path(\"./index/default__vector_store.json\")\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    vector_store=vector_store, persist_dir=\"./index\"\n",
    ")\n",
    "retrieved_index = load_index_from_storage(storage_context=storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 690be0ea-28f2-4c64-88a5-a1fab2095c5f<br>**Similarity:** 326.43353271484375<br>**Text:** The AI winter(s) did not happen suddenly. There were some radical voices that predicted it like that of Hubert Dreyfus. They theorised that the magical activities like object recognition are not symbol manipulation problems. These are perhaps pattern matching problems. So here is our third category and lets call them the “thinking = pattern sifting” school. Thanks to them, we learnt that we can’t solve a problem of recognising an object, say a chair, even if we write thousands of rules. (Although its a core component of intelligence, is recognition a sufficient capability to create thinking machines? Let us come back to this shortly)\n",
       "\n",
       "While progressing in parallel on the natural language processing frontier, we have recently landed on another possible solution to thinking machines — the LLMs. This is a variation of the pattern sifting approach, applied to word patterns in languages. By mastering the language syntax patterns sourced from an unlimited corpus and by using them as references to generate eloquent statements, these models seem to simulate thinking. This is the claim of the fourth school and lets call them the “thinking = sequence transduction” school. This also would mean that people perhaps are nothing but evolved sequence models.\n",
       "\n",
       "I am not going to dive into the second category for now.<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 1a1af6e9-9da2-435f-a067-25713489c386<br>**Similarity:** 328.9163513183594<br>**Text:** When I was an entrepreneur about a decade ago, I landed on a golden use case (or so I fantasised). It is to apply Natural Language Processing (NLP) to convert english statements into structured facts (Subject-Predicate-Object) that adhere to commonly-agreed Domain Ontologies. For example, “Singapore’s economic inflation is estimated at 4.5%” can be expressed as a triple with Singapore as the subject, 4.5 as the object and has_inflation as the predicate (semantic relatioship). All the Subjects, Objects and Predicates can be expressed as unique URIs so that the knowledge across millions of triples describing different aspects of the same subjects and objects can be stictched together. Such fact triples can be loaded onto Semantic Datastores (RDF or Property-Graph based) and queried, again by the same natural language processing capability. Even more so, these triples can be crunched by inferencing engines to derive semantically sound new knowledge and can reason these derivations in reverse too. The ultimate thinking machine! or so I fantasised.\n",
       "\n",
       "Why this seemingly round-about approach to create a thinking machine as against the generative AI’s approach of predicting word-sequences? Why convert unstructured English to structured facts before processing it for an answer? And are there any other alternative approaches to creating thinking machines? Strangely, I have not heard much balanced critical opinions from contemporary computing historians or philosophers on this subject ever since LLMs were made public. It could also be that I am living under a certain other rock but all I hear is the whole industry monomaniacally adulating the genAI’s auto-regressive model, bullishly sidelining any reasonable criticisms. To correctly quantify the pleasantly surprising merits and some devastating demerits, we need to philosophically ponder on the humanity’s past quests on this matter and re-learn from it.<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "retriever = retrieved_index.as_retriever()\n",
    "response = retriever.retrieve(\n",
    "    \"How can semantic datastores be used in reasoning\"\n",
    ")\n",
    "for n in response:\n",
    "    display_source_node(n, source_length=20000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genxai",
   "language": "python",
   "name": "genxai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
